{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clearer visualization of feature matches (instead of the colorful grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import gc\n",
    "import time\n",
    "from typing import Any\n",
    "import sys\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils._pytree import tree_map\n",
    "import torch_levenberg_marquardt as tlm\n",
    "from torchmetrics import Metric\n",
    "import roma\n",
    "from tqdm import tqdm\n",
    "import visu3d as v3d\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(\"external\", \"bop_toolkit\")))\n",
    "sys.path.append(os.path.abspath(os.path.join(\"external\", \"dinov2\")))\n",
    "from bop_toolkit_lib import inout\n",
    "\n",
    "from utils.misc import array_to_tensor, tensor_to_array, tensors_to_arrays\n",
    "from utils import (\n",
    "    corresp_util,\n",
    "    eval_util,\n",
    "    feature_util,\n",
    "    knn_util,\n",
    "    misc as misc_util,\n",
    "    pnp_util,\n",
    "    projector_util,\n",
    "    repre_util,\n",
    "    vis_util,\n",
    "    renderer_builder,\n",
    "    json_util, \n",
    "    logging,\n",
    "    misc,\n",
    "    structs,\n",
    "    geometry,\n",
    ")\n",
    "from utils.structs import AlignedBox2f, PinholePlaneCameraModel, CameraModel\n",
    "from utils.misc import warp_image\n",
    "from utils.run.opts import CommonOpts, InferOpts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgpath = Path(\"/scratch/jeyan/barreldata/divedata/barrelddt1/rgb/cropped0000.png\")\n",
    "maskpath = Path(\"/scratch/jeyan/barreldata/results/barrelddt1/sam-masks/cropped0000.png\")\n",
    "tmpdir = Path(\"/scratch/jeyan/barreldata/tmp\")\n",
    "\n",
    "cam_json_path = Path(\"/scratch/jeyan/barreldata/divedata/barrelddt1/camera.json\")\n",
    "\n",
    "device = \"cuda:1\"\n",
    "\n",
    "extractor_name = \"dinov2_version=vitl14-reg_stride=14_facet=token_layer=18_logbin=0_norm=1\"\n",
    "extractor = feature_util.make_feature_extractor(extractor_name)\n",
    "extractor.to(device)\n",
    "\n",
    "repre_dir = Path(\"/scratch/jeyan/barreldata/results/barrelddt1/foundpose-output/object_repre\")\n",
    "repre = repre_util.load_object_repre(\n",
    "    repre_dir=repre_dir,\n",
    "    tensor_device=device,\n",
    ")\n",
    "repre_np = repre_util.convert_object_repre_to_numpy(repre)\n",
    "\n",
    "# Build a kNN index from object feature vectors.\n",
    "visual_words_knn_index = None\n",
    "visual_words_knn_index = knn_util.KNN(\n",
    "    k=repre.template_desc_opts.tfidf_knn_k,\n",
    "    metric=repre.template_desc_opts.tfidf_knn_metric\n",
    ")\n",
    "visual_words_knn_index.fit(repre.feat_cluster_centroids)\n",
    "\n",
    "# Build per-template KNN index with features from that template.\n",
    "template_knn_indices = []\n",
    "for template_id in range(len(repre.template_cameras_cam_from_model)):\n",
    "    tpl_feat_mask = repre.feat_to_template_ids == template_id\n",
    "    tpl_feat_ids = torch.nonzero(tpl_feat_mask).flatten()\n",
    "\n",
    "    template_feats = repre.feat_vectors[tpl_feat_ids]\n",
    "\n",
    "    # Build knn index for object features.\n",
    "    template_knn_index = knn_util.KNN(k=1, metric=\"l2\")\n",
    "    template_knn_index.fit(template_feats.cpu())\n",
    "    template_knn_indices.append(template_knn_index)\n",
    "\n",
    "# Camera parameters.\n",
    "# transform is from GT, can we just leave as identity?\n",
    "with open(Path(cam_json_path), \"r\") as f:\n",
    "    camjson = json.load(f)\n",
    "orig_camera_c2w = PinholePlaneCameraModel(\n",
    "    camjson[\"width\"], camjson[\"height\"],\n",
    "    (camjson[\"fx\"], camjson[\"fy\"]), (camjson[\"cx\"], camjson[\"cy\"])\n",
    ")\n",
    "orig_image_size = (\n",
    "    orig_camera_c2w.width,\n",
    "    orig_camera_c2w.height,\n",
    ")\n",
    "\n",
    "# Generate grid points at which to sample the feature vectors.\n",
    "grid_cell_size = 14\n",
    "crop_size = (420, 420)\n",
    "grid_size = crop_size\n",
    "crop_rel_pad = 0.2\n",
    "grid_points = feature_util.generate_grid_points(\n",
    "    grid_size=grid_size,\n",
    "    cell_size=grid_cell_size,\n",
    ")\n",
    "grid_points = grid_points.to(device)\n",
    "\n",
    "# Estimate pose for each object instance.\n",
    "# Get the input image.\n",
    "orig_image_np_hwc = np.array(Image.open(imgpath)) / 255.0\n",
    "\n",
    "# Get the modal mask and amodal bounding box of the instance.\n",
    "# binary mask\n",
    "orig_mask_modal = np.array(Image.open(maskpath).convert(\"L\")) / 255.0\n",
    "sumvert = np.sum(orig_mask_modal, axis=0)\n",
    "left = np.where(sumvert > 0)[0][0]\n",
    "right = np.where(sumvert > 0)[0][-1]\n",
    "sumhor = np.sum(orig_mask_modal, axis=1)\n",
    "bottom = np.where(sumhor > 0)[0][0]\n",
    "top = np.where(sumhor > 0)[0][-1]\n",
    "# bounding box of mask\n",
    "orig_box_amodal = AlignedBox2f(\n",
    "    left=left,\n",
    "    top=top,\n",
    "    right=right,\n",
    "    bottom=bottom,\n",
    ")\n",
    "\n",
    "# Optional cropping.\n",
    "# Get box for cropping.\n",
    "crop_box = misc_util.calc_crop_box(\n",
    "    box=orig_box_amodal,\n",
    "    make_square=True,\n",
    ")\n",
    "\n",
    "# Construct a virtual camera focused on the crop.\n",
    "crop_camera_model_c2w = misc_util.construct_crop_camera(\n",
    "    box=crop_box,\n",
    "    camera_model_c2w=orig_camera_c2w,\n",
    "    viewport_size=crop_size,\n",
    "    viewport_rel_pad=crop_rel_pad,\n",
    ")\n",
    "\n",
    "# Map images to the virtual camera.\n",
    "interpolation = (\n",
    "    cv2.INTER_AREA\n",
    "    if crop_box.width >= crop_camera_model_c2w.width\n",
    "    else cv2.INTER_LINEAR\n",
    ")\n",
    "image_np_hwc = warp_image(\n",
    "    src_camera=orig_camera_c2w,\n",
    "    dst_camera=crop_camera_model_c2w,\n",
    "    src_image=orig_image_np_hwc,\n",
    "    interpolation=interpolation,\n",
    ")\n",
    "mask_modal = warp_image(\n",
    "    src_camera=orig_camera_c2w,\n",
    "    dst_camera=crop_camera_model_c2w,\n",
    "    src_image=orig_mask_modal,\n",
    "    interpolation=cv2.INTER_NEAREST,\n",
    ")\n",
    "\n",
    "# Recalculate the object bounding box (it changed if we constructed the virtual camera).\n",
    "ys, xs = mask_modal.nonzero()\n",
    "box = np.array(misc_util.calc_2d_box(xs, ys))\n",
    "box_amodal = AlignedBox2f(\n",
    "    left=box[0],\n",
    "    top=box[1],\n",
    "    right=box[2],\n",
    "    bottom=box[3],\n",
    ")\n",
    "\n",
    "# The virtual camera is becoming the main camera.\n",
    "camera_c2w = crop_camera_model_c2w\n",
    "\n",
    "\n",
    "# Extract feature map from the crop.\n",
    "image_tensor_chw = array_to_tensor(image_np_hwc).to(torch.float32).permute(2, 0, 1).to(device)\n",
    "image_tensor_bchw = image_tensor_chw.unsqueeze(0)\n",
    "# BxDxHxW\n",
    "extractor_output = extractor(image_tensor_bchw)\n",
    "feature_map_chw = extractor_output[\"feature_maps\"][0]\n",
    "\n",
    "\n",
    "# Keep only points inside the object mask.\n",
    "mask_modal_tensor = array_to_tensor(mask_modal).to(device)\n",
    "query_points = feature_util.filter_points_by_mask(\n",
    "    grid_points, mask_modal_tensor\n",
    ")\n",
    "\n",
    "# Subsample query points if we have too many.\n",
    "max_num_queries = 1000000\n",
    "if query_points.shape[0] > max_num_queries:\n",
    "    perm = torch.randperm(query_points.shape[0])\n",
    "    query_points = query_points[perm[: max_num_queries]]\n",
    "    msg = (\n",
    "        \"Randomly sumbsampled queries \"\n",
    "        f\"({perm.shape[0]} -> {query_points.shape[0]}))\"\n",
    "    )\n",
    "\n",
    "# Extract features at the selected points, of shape (num_points, feat_dims).\n",
    "query_features = feature_util.sample_feature_map_at_points(\n",
    "    feature_map_chw=feature_map_chw,\n",
    "    points=query_points,\n",
    "    image_size=(image_np_hwc.shape[1], image_np_hwc.shape[0]),\n",
    ").contiguous()\n",
    "\n",
    "# Potentially project features to a PCA space.\n",
    "if (\n",
    "    query_features.shape[1] != repre.feat_vectors.shape[1]\n",
    "    and len(repre.feat_raw_projectors) != 0\n",
    "):\n",
    "    query_features_proj = projector_util.project_features(\n",
    "        feat_vectors=query_features,\n",
    "        projectors=repre.feat_raw_projectors,\n",
    "    ).contiguous()\n",
    "\n",
    "    _c, _h, _w = feature_map_chw.shape\n",
    "    feature_map_chw_proj = (\n",
    "        projector_util.project_features(\n",
    "            feat_vectors=feature_map_chw.permute(1, 2, 0).view(-1, _c),\n",
    "            projectors=repre.feat_raw_projectors,\n",
    "        )\n",
    "        .view(_h, _w, -1)\n",
    "        .permute(2, 0, 1)\n",
    "    )\n",
    "else:\n",
    "    query_features_proj = query_features\n",
    "    feature_map_chw_proj = feature_map_chw\n",
    "\n",
    "\n",
    "# Establish 2D-3D correspondences.\n",
    "allcorresp = corresp_util.establish_correspondences(\n",
    "    query_points=query_points,\n",
    "    query_features=query_features_proj,\n",
    "    object_repre=repre,\n",
    "    template_matching_type=\"tfidf\",\n",
    "    template_knn_indices=template_knn_indices,\n",
    "    feat_matching_type=\"cyclic_buddies\",\n",
    "    top_n_templates=5,\n",
    "    top_k_buddies=300,\n",
    "    visual_words_knn_index=visual_words_knn_index,\n",
    "    debug=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corresp = allcorresp[0]\n",
    "corresp = tensors_to_arrays(corresp)\n",
    "template_id = corresp[\"template_id\"]\n",
    "object_repre = repre_np\n",
    "corresp_top_n = 100\n",
    "# Get left 2D points.\n",
    "# left: rgb image\n",
    "selected_ids = corresp[\"coord_conf\"].argsort()[::-1][:corresp_top_n]\n",
    "kpts_left = corresp[\"coord_2d\"][selected_ids]\n",
    "\n",
    "# Get right 2D points.\n",
    "# right: template image\n",
    "tpl_cameras_m2c = object_repre.template_cameras_cam_from_model[template_id]\n",
    "all_tpl_vertex_ids = corresp[\"nn_vertex_ids\"]\n",
    "all_tpl_vertices_in_c = geometry.transform_3d_points_numpy(\n",
    "    np.linalg.inv(tpl_cameras_m2c.T_world_from_eye), object_repre.vertices[all_tpl_vertex_ids]\n",
    ")\n",
    "all_kpts_right = tpl_cameras_m2c.eye_to_window(all_tpl_vertices_in_c)\n",
    "kpts_right = all_kpts_right[selected_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kpts_right, kpts_left = kpts_right.astype(int), kpts_left.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leftimg = (image_np_hwc * 255).astype(np.uint8)\n",
    "rightimg = repre_np.templates[template_id].transpose(1, 2, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = np.random.randint(0, 255, (10, 3)).tolist()\n",
    "colors = [\n",
    "    [176, 67, 0],\n",
    "    [107, 23, 135],\n",
    "    [24, 21, 179],\n",
    "    [19, 89, 11],\n",
    "    [82, 56, 9],\n",
    "    [22, 110, 99],\n",
    "    [166, 41, 153],\n",
    "    [135, 110, 36],\n",
    "    [108, 128, 46],\n",
    "    [163, 128, 59],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "both = np.concatenate((leftimg, rightimg), axis=1)\n",
    "npoints = 10\n",
    "for i, (kptleft, kptright) in enumerate(zip(kpts_left[::20], kpts_right[::20])):\n",
    "    color = colors[i]\n",
    "    both = cv2.line(both, (kptleft[0], kptleft[1]), (kptright[0] + leftimg.shape[1], kptright[1]), [255, 255, 255], 5)\n",
    "    both = cv2.line(both, (kptleft[0], kptleft[1]), (kptright[0] + leftimg.shape[1], kptright[1]), color, 3)\n",
    "    both = cv2.circle(both, (kptleft[0], kptleft[1]), 8, [255, 255, 255], -1)\n",
    "    both = cv2.circle(both, (kptleft[0], kptleft[1]), 6, color, -1)\n",
    "    both = cv2.circle(both, (kptright[0] + leftimg.shape[1], kptright[1]), 8, [255, 255, 255], -1)\n",
    "    both = cv2.circle(both, (kptright[0] + leftimg.shape[1], kptright[1]), 6, color, -1)\n",
    "tmp = Image.fromarray(both)\n",
    "tmp.save(tmpdir / \"corresp_dino.png\")\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate SIFT detector\n",
    "sift = cv2.SIFT_create(nfeatures=60, contrastThreshold=0.01, edgeThreshold=None)\n",
    " \n",
    "# find the keypoints and descriptors with SIFT\n",
    "kp1, des1 = sift.detectAndCompute(leftimg, mask=(mask_modal * 255).astype(np.uint8))\n",
    "kp2, des2 = sift.detectAndCompute(rightimg, mask=((cv2.cvtColor(rightimg, cv2.COLOR_RGB2GRAY) > 0) * 255).astype(np.uint8))\n",
    " \n",
    "# BFMatcher with default params\n",
    "bf = cv2.BFMatcher()\n",
    "matches = bf.knnMatch(des1, des2, k=2)\n",
    " \n",
    "# Apply ratio test\n",
    "good = []\n",
    "for m,n in matches:\n",
    "    # print(m.distance, n.distance)\n",
    "    if m.distance < 0.95*n.distance:\n",
    "        good.append([m])\n",
    " \n",
    "# cv.drawMatchesKnn expects list of lists as matches.\n",
    "# img3 = cv2.drawMatchesKnn(leftimg,kp1,rightimg,kp2,good,None,flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "# Image.fromarray(img3).save(tmpdir / \"corresp_sift.png\")\n",
    "both = np.concatenate((leftimg, rightimg), axis=1)\n",
    "for i, goodmatch in enumerate(good[7:12]):\n",
    "    color = colors[i]\n",
    "    kptleft = kp1[goodmatch[0].queryIdx].pt\n",
    "    kptleft = (int(kptleft[0]), int(kptleft[1]))\n",
    "    kptright = kp2[goodmatch[0].trainIdx].pt\n",
    "    kptright = (int(kptright[0]), int(kptright[1]))\n",
    "    both = cv2.line(both, (kptleft[0], kptleft[1]), (kptright[0] + leftimg.shape[1], kptright[1]), [255, 255, 255], 5)\n",
    "    both = cv2.line(both, (kptleft[0], kptleft[1]), (kptright[0] + leftimg.shape[1], kptright[1]), color, 3)\n",
    "    both = cv2.circle(both, (kptleft[0], kptleft[1]), 8, [255, 255, 255], -1)\n",
    "    both = cv2.circle(both, (kptleft[0], kptleft[1]), 6, color, -1)\n",
    "    both = cv2.circle(both, (kptright[0] + leftimg.shape[1], kptright[1]), 8, [255, 255, 255], -1)\n",
    "    both = cv2.circle(both, (kptright[0] + leftimg.shape[1], kptright[1]), 6, color, -1)\n",
    "tmp = Image.fromarray(both)\n",
    "tmp.save(tmpdir / \"corresp_sift.png\")\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "foundpose_gpu_311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
