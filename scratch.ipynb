{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import gc\n",
    "import time\n",
    "import struct\n",
    "\n",
    "from typing import List, NamedTuple, Optional, Tuple\n",
    "\n",
    "import cv2\n",
    "import imageio\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(os.path.join(\"external\", \"bop_toolkit\")))\n",
    "from bop_toolkit_lib import inout\n",
    "sys.path.append(os.path.abspath(os.path.join(\"external\", \"dinov2\")))\n",
    "\n",
    "from utils.misc import array_to_tensor, tensor_to_array, tensors_to_arrays\n",
    "\n",
    "\n",
    "from utils import (\n",
    "    corresp_util,\n",
    "    config_util,\n",
    "    eval_errors,\n",
    "    eval_util,\n",
    "    feature_util,\n",
    "    infer_pose_util,\n",
    "    knn_util,\n",
    "    misc as misc_util,\n",
    "    pnp_util,\n",
    "    projector_util,\n",
    "    repre_util,\n",
    "    vis_util,\n",
    "    data_util,\n",
    "    renderer_builder,\n",
    "    json_util, \n",
    "    logging,\n",
    "    misc,\n",
    "    structs,\n",
    ")\n",
    "\n",
    "from utils.structs import AlignedBox2f, PinholePlaneCameraModel, CameraModel\n",
    "from utils.misc import warp_depth_image, warp_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor_name = \"dinov2_version=vitl14-reg_stride=14_facet=token_layer=18_logbin=0_norm=1\"\n",
    "output_path = Path(\"/scratch/jeyan/foundpose/output_barrelddt1_raw_vitl_layer18\")\n",
    "dataset_path = Path(\"/scratch/jeyan/barreldata/divedata/dive8/barrelddt1/rgb\")\n",
    "mask_path = Path(\"/scratch/jeyan/barreldata/results/barrelddt1/masks\")\n",
    "cam_json_path = Path(\"/scratch/jeyan/barreldata/divedata/dive8/barrelddt1/camera.json\")\n",
    "crop_size = (420, 420)\n",
    "grid_cell_size = 14.0\n",
    "crop_rel_pad = 0.2\n",
    "\n",
    "imgpaths = sorted(list(dataset_path.glob(\"*.jpg\")) + list(dataset_path.glob(\"*.png\")))\n",
    "maskpaths = sorted(list(mask_path.glob(\"*.png\")))\n",
    "\n",
    "# Prepare feature extractor.\n",
    "extractor = feature_util.make_feature_extractor(extractor_name)\n",
    "# Prepare a device.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "extractor.to(device)\n",
    "\n",
    "# Create a pose evaluator.\n",
    "pose_evaluator = eval_util.EvaluatorPose([0])\n",
    "\n",
    "# Load the object representation.\n",
    "# logger.info(\n",
    "#     f\"Loading representation for object {0} from dataset {opts.object_dataset}...\"\n",
    "# )\n",
    "base_repre_dir = Path(output_path, \"object_repre\")\n",
    "repre_dir = base_repre_dir\n",
    "repre: repre_util.FeatureBasedObjectRepre = repre_util.load_object_repre(\n",
    "    repre_dir=repre_dir,\n",
    "    tensor_device=device,\n",
    ")\n",
    "\n",
    "repre_np = repre_util.convert_object_repre_to_numpy(repre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repre_np.feat_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feattempmask = repre_np.feat_to_template_ids == 0\n",
    "# all p_i values\n",
    "repre_np.feat_vectors[feattempmask].shape\n",
    "# all x_i values\n",
    "repre_np.vertices[repre_np.feat_to_vertex_ids[feattempmask]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repre_np.vertices[repre_np.feat_to_vertex_ids[feattempmask]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "imgpath = imgpaths[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Camera parameters.\n",
    "# transform is from GT, can we just leave as identity?\n",
    "with open(Path(cam_json_path), \"r\") as f:\n",
    "    camjson = json.load(f)\n",
    "orig_camera_c2w = PinholePlaneCameraModel(\n",
    "    camjson[\"width\"], camjson[\"height\"],\n",
    "    (camjson[\"fx\"], camjson[\"fy\"]), (camjson[\"cx\"], camjson[\"cy\"])\n",
    ")\n",
    "orig_image_size = (\n",
    "    orig_camera_c2w.width,\n",
    "    orig_camera_c2w.height,\n",
    ")\n",
    "\n",
    "# Generate grid points at which to sample the feature vectors.\n",
    "grid_size = crop_size\n",
    "grid_points = feature_util.generate_grid_points(\n",
    "    grid_size=grid_size,\n",
    "    cell_size=grid_cell_size,\n",
    ")\n",
    "grid_points = grid_points.to(device)\n",
    "\n",
    "\n",
    "# Estimate pose for each object instance.\n",
    "times = {}\n",
    "\n",
    "# Get the input image.\n",
    "orig_image_np_hwc = np.array(Image.open(imgpath)) / 255.0\n",
    "\n",
    "# Get the modal mask and amodal bounding box of the instance.\n",
    "# binary mask\n",
    "orig_mask_modal = np.array(Image.open(maskpaths[i]).convert(\"L\")) / 255.0\n",
    "sumvert = np.sum(orig_mask_modal, axis=0)\n",
    "left = np.where(sumvert > 0)[0][0]\n",
    "right = np.where(sumvert > 0)[0][-1]\n",
    "sumhor = np.sum(orig_mask_modal, axis=1)\n",
    "bottom = np.where(sumhor > 0)[0][0]\n",
    "top = np.where(sumhor > 0)[0][-1]\n",
    "# bounding box of mask\n",
    "orig_box_amodal = AlignedBox2f(\n",
    "    left=left,\n",
    "    top=top,\n",
    "    right=right,\n",
    "    bottom=bottom,\n",
    ")\n",
    "\n",
    "# Get box for cropping.\n",
    "crop_box = misc_util.calc_crop_box(\n",
    "    box=orig_box_amodal,\n",
    "    make_square=True,\n",
    ")\n",
    "\n",
    "# Construct a virtual camera focused on the crop.\n",
    "crop_camera_model_c2w = misc_util.construct_crop_camera(\n",
    "    box=crop_box,\n",
    "    camera_model_c2w=orig_camera_c2w,\n",
    "    viewport_size=crop_size,\n",
    "    viewport_rel_pad=crop_rel_pad,\n",
    ")\n",
    "\n",
    "# Map images to the virtual camera.\n",
    "interpolation = (\n",
    "    cv2.INTER_AREA\n",
    "    if crop_box.width >= crop_camera_model_c2w.width\n",
    "    else cv2.INTER_LINEAR\n",
    ")\n",
    "image_np_hwc = warp_image(\n",
    "    src_camera=orig_camera_c2w,\n",
    "    dst_camera=crop_camera_model_c2w,\n",
    "    src_image=orig_image_np_hwc,\n",
    "    interpolation=interpolation,\n",
    ")\n",
    "mask_modal = warp_image(\n",
    "    src_camera=orig_camera_c2w,\n",
    "    dst_camera=crop_camera_model_c2w,\n",
    "    src_image=orig_mask_modal,\n",
    "    interpolation=cv2.INTER_NEAREST,\n",
    ")\n",
    "\n",
    "# Recalculate the object bounding box (it changed if we constructed the virtual camera).\n",
    "ys, xs = mask_modal.nonzero()\n",
    "box = np.array(misc_util.calc_2d_box(xs, ys))\n",
    "box_amodal = AlignedBox2f(\n",
    "    left=box[0],\n",
    "    top=box[1],\n",
    "    right=box[2],\n",
    "    bottom=box[3],\n",
    ")\n",
    "\n",
    "# The virtual camera is becoming the main camera.\n",
    "camera_c2w = crop_camera_model_c2w\n",
    "\n",
    "# Extract feature map from the crop.\n",
    "image_tensor_chw = array_to_tensor(image_np_hwc).to(torch.float32).permute(2, 0, 1).to(device)\n",
    "image_tensor_bchw = image_tensor_chw.unsqueeze(0)\n",
    "# BxDxHxW\n",
    "extractor_output = extractor(image_tensor_bchw)\n",
    "feature_map_chw = extractor_output[\"feature_maps\"][0]\n",
    "\n",
    "# Keep only points inside the object mask.\n",
    "mask_modal_tensor = array_to_tensor(mask_modal).to(device)\n",
    "query_points = feature_util.filter_points_by_mask(\n",
    "    grid_points, mask_modal_tensor\n",
    ")\n",
    "\n",
    "# Extract features at the selected points, of shape (num_points, feat_dims).\n",
    "query_features = feature_util.sample_feature_map_at_points(\n",
    "    feature_map_chw=feature_map_chw,\n",
    "    points=query_points,\n",
    "    image_size=(image_np_hwc.shape[1], image_np_hwc.shape[0]),\n",
    ").contiguous()\n",
    "\n",
    "# Potentially project features to a PCA space.\n",
    "if (\n",
    "    query_features.shape[1] != repre.feat_vectors.shape[1]\n",
    "    and len(repre.feat_raw_projectors) != 0\n",
    "):\n",
    "    query_features_proj = projector_util.project_features(\n",
    "        feat_vectors=query_features,\n",
    "        projectors=repre.feat_raw_projectors,\n",
    "    ).contiguous()\n",
    "\n",
    "    _c, _h, _w = feature_map_chw.shape\n",
    "    feature_map_chw_proj = (\n",
    "        projector_util.project_features(\n",
    "            feat_vectors=feature_map_chw.permute(1, 2, 0).view(-1, _c),\n",
    "            projectors=repre.feat_raw_projectors,\n",
    "        )\n",
    "        .view(_h, _w, -1)\n",
    "        .permute(2, 0, 1)\n",
    "    )\n",
    "else:\n",
    "    query_features_proj = query_features\n",
    "    feature_map_chw_proj = feature_map_chw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_map_chw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_features_proj.shape, feature_map_chw_proj.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fq = feature_map_chw_proj.permute(1, 2, 0).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bilinear_interpolate(im, x, y):\n",
    "    \"\"\"Copilot take the wheel.\"\"\"\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    x0 = np.floor(x).astype(np.int64)\n",
    "    x1 = x0 + 1\n",
    "    y0 = np.floor(y).astype(np.int64)\n",
    "    y1 = y0 + 1\n",
    "\n",
    "    x0 = np.clip(x0, 0, im.shape[1] - 1)\n",
    "    x1 = np.clip(x1, 0, im.shape[1] - 1)\n",
    "    y0 = np.clip(y0, 0, im.shape[0] - 1)\n",
    "    y1 = np.clip(y1, 0, im.shape[0] - 1)\n",
    "\n",
    "    Ia = im[y0, x0]\n",
    "    Ib = im[y1, x0]\n",
    "    Ic = im[y0, x1]\n",
    "    Id = im[y1, x1]\n",
    "\n",
    "    wa = (x1 - x) * (y1 - y)\n",
    "    wb = (x1 - x) * (y - y0)\n",
    "    wc = (x - x0) * (y1 - y)\n",
    "    wd = (x - x0) * (y - y0)\n",
    "\n",
    "    return (wa * Ia + wb * Ib + wc * Ic + wd * Id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = np.array([\n",
    "    [0.9824963517866108, -0.18626298081482784, -0.0026496611057800646],\n",
    "    [0.07184429208626533, 0.36576270685794776, 0.9279310534552504],\n",
    "    [-0.1718700567889119, -0.9118792377557032, 0.37274245710604814]\n",
    "])\n",
    "t = np.array([0.027086201383687152, 0.21964677626568319, 5.018856479067855])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_id = 245\n",
    "feattempmask = repre_np.feat_to_template_ids == template_id\n",
    "# all p_i values\n",
    "allpi = repre_np.feat_vectors[feattempmask]\n",
    "# all x_i values\n",
    "allxi = repre_np.vertices[repre_np.feat_to_vertex_ids[feattempmask]]\n",
    "Fq = feature_map_chw_proj.permute(1, 2, 0).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def robustcost(x, a=-5, c=0.5):\n",
    "    return sum((abs(a - 2) / a) * (((x / c) ** 2 / abs(a - 2) + 1) ** (a / 2) - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = allpi[0]\n",
    "xi = allxi[0]\n",
    "x, y = camera_c2w.eye_to_window(R @ xi + t) / 14\n",
    "robustcost(pi - bilinear_interpolate(Fq, x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def axangle2mat(axis, angle=None):\n",
    "    \"\"\"\n",
    "    Rotation matrix for rotation angle `angle` around `axis`.\n",
    "    Ripped from transforms3d with some tweaks.\n",
    "\n",
    "    From: http://en.wikipedia.org/wiki/Rotation_matrix#Axis_and_angle\n",
    "\n",
    "    Args:\n",
    "        axis (3 element sequence): vector specifying axis for rotation.\n",
    "        angle (scalar): angle of rotation in radians. Default is the norm of `axis`.\n",
    "\n",
    "    Returns\n",
    "        mat (array shape (3,3)): rotation matrix for specified rotation\n",
    "    \"\"\"\n",
    "    x, y, z = axis\n",
    "    n = math.sqrt(x*x + y*y + z*z)\n",
    "    x = x / n\n",
    "    y = y / n\n",
    "    z = z / n\n",
    "    if angle is None:\n",
    "        angle = n\n",
    "    c = math.cos(angle); s = math.sin(angle); C = 1-c\n",
    "    xs = x*s;   ys = y*s;   zs = z*s\n",
    "    xC = x*C;   yC = y*C;   zC = z*C\n",
    "    xyC = x*yC; yzC = y*zC; zxC = z*xC\n",
    "    return np.array([\n",
    "            [ x*xC+c,   xyC-zs,   zxC+ys ],\n",
    "            [ xyC+zs,   y*yC+c,   yzC-xs ],\n",
    "            [ zxC-ys,   yzC+xs,   z*zC+c ]])\n",
    "\n",
    "def mat2axangle(mat, unit_thresh=1e-5):\n",
    "    \"\"\"Return axis, angle and point from (3, 3) matrix `mat`\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    mat : array-like shape (3, 3)\n",
    "        Rotation matrix\n",
    "    unit_thresh : float, optional\n",
    "        Tolerable difference from 1 when testing for unit eigenvalues to\n",
    "        confirm `mat` is a rotation matrix.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    axis : array shape (3,)\n",
    "       vector giving axis of rotation\n",
    "    angle : scalar\n",
    "       angle of rotation in radians.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> direc = np.random.random(3) - 0.5\n",
    "    >>> angle = (np.random.random() - 0.5) * (2*math.pi)\n",
    "    >>> R0 = axangle2mat(direc, angle)\n",
    "    >>> direc, angle = mat2axangle(R0)\n",
    "    >>> R1 = axangle2mat(direc, angle)\n",
    "    >>> np.allclose(R0, R1)\n",
    "    True\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    http://en.wikipedia.org/wiki/Rotation_matrix#Axis_of_a_rotation\n",
    "    \"\"\"\n",
    "    M = np.asarray(mat, dtype=np.float64)\n",
    "    # direction: unit eigenvector of R33 corresponding to eigenvalue of 1\n",
    "    L, W = np.linalg.eig(M.T)\n",
    "    i = np.where(np.abs(L - 1.0) < unit_thresh)[0]\n",
    "    if not len(i):\n",
    "        raise ValueError(\"no unit eigenvector corresponding to eigenvalue 1\")\n",
    "    direction = np.real(W[:, i[-1]]).squeeze()\n",
    "    # rotation angle depending on direction\n",
    "    cosa = (np.trace(M) - 1.0) / 2.0\n",
    "    if abs(direction[2]) > 1e-8:\n",
    "        sina = (M[1, 0] + (cosa-1.0)*direction[0]*direction[1]) / direction[2]\n",
    "    elif abs(direction[1]) > 1e-8:\n",
    "        sina = (M[0, 2] + (cosa-1.0)*direction[0]*direction[2]) / direction[1]\n",
    "    else:\n",
    "        sina = (M[2, 1] + (cosa-1.0)*direction[1]*direction[2]) / direction[0]\n",
    "    angle = math.atan2(sina, cosa)\n",
    "    return direction, angle\n",
    "\n",
    "def featuremetric_loss(tfm, camera, patchdescs, patchvtxs, featmap, patchsize, a=-5, c=0.5):\n",
    "    # need: projected features from every patch in query rgb image\n",
    "    # template descriptor + 3d point from patches inside mask\n",
    "    # coarse R and t, need to parametrize R as quaternion or axangle\n",
    "    # tfm is 6d [*axangle, *t]\n",
    "    npatches = len(patchdescs)\n",
    "    losses = np.zeros(npatches)\n",
    "    axangle = tfm[:3]\n",
    "    R = axangle2mat(axangle)\n",
    "    t = tfm[3:]\n",
    "    totalerror = 0.0\n",
    "    for i, pi in enumerate(patchdescs):\n",
    "        xi = patchvtxs[i]\n",
    "        x, y = camera.eye_to_window(R @ xi + t) / patchsize\n",
    "        losses[i] = robustcost(pi - bilinear_interpolate(featmap, x, y), a=a, c=c)\n",
    "    return losses\n",
    "\n",
    "def featuremetric_error(tfm, camera, patchdescs, patchvtxs, featmap, patchsize, a=-5, c=0.5):\n",
    "    # need: projected features from every patch in query rgb image\n",
    "    # template descriptor + 3d point from patches inside mask\n",
    "    # coarse R and t, need to parametrize R as quaternion or axangle\n",
    "    # tfm is 6d [*axangle, *t]\n",
    "    axangle = tfm[:3]\n",
    "    R = axangle2mat(axangle)\n",
    "    t = tfm[3:]\n",
    "    totalerror = 0.0\n",
    "    for i, pi in enumerate(patchdescs):\n",
    "        xi = patchvtxs[i]\n",
    "        x, y = camera.eye_to_window(R @ xi + t) / patchsize\n",
    "        totalerror += robustcost(pi - bilinear_interpolate(featmap, x, y), a=a, c=c)\n",
    "    return totalerror"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axang, theta = mat2axangle(R)\n",
    "axang = axang * theta\n",
    "tfm = np.concatenate([axang, t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import least_squares, minimize\n",
    "featuremetric_error(tfm, camera_c2w, allpi, allxi, Fq, 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "least_squares(featuremetric_loss, tfm, args=(camera_c2w, allpi, allxi, Fq, 14), method=\"lm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "foundpose_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
