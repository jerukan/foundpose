{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "yeah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import gc\n",
    "import time\n",
    "import struct\n",
    "\n",
    "from typing import List, NamedTuple, Optional, Tuple, Any\n",
    "\n",
    "import cv2\n",
    "import imageio\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils._pytree import tree_map\n",
    "from torchmetrics import Metric\n",
    "import roma\n",
    "import torch_levenberg_marquardt as tlm\n",
    "import matplotlib.pyplot as plt\n",
    "import visu3d as v3d\n",
    "\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(os.path.join(\"external\", \"bop_toolkit\")))\n",
    "from bop_toolkit_lib import inout\n",
    "sys.path.append(os.path.abspath(os.path.join(\"external\", \"dinov2\")))\n",
    "\n",
    "from utils.misc import array_to_tensor, tensor_to_array, tensors_to_arrays\n",
    "\n",
    "\n",
    "from utils import (\n",
    "    corresp_util,\n",
    "    config_util,\n",
    "    eval_errors,\n",
    "    eval_util,\n",
    "    feature_util,\n",
    "    infer_pose_util,\n",
    "    knn_util,\n",
    "    misc as misc_util,\n",
    "    pnp_util,\n",
    "    projector_util,\n",
    "    repre_util,\n",
    "    vis_util,\n",
    "    data_util,\n",
    "    renderer_builder,\n",
    "    json_util, \n",
    "    logging,\n",
    "    misc,\n",
    "    structs,\n",
    "    geometry\n",
    ")\n",
    "\n",
    "from utils.structs import AlignedBox2f, PinholePlaneCameraModel, CameraModel\n",
    "from utils.misc import warp_depth_image, warp_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor_name = \"dinov2_version=vitl14-reg_stride=14_facet=token_layer=18_logbin=0_norm=1\"\n",
    "output_path = Path(\"/scratch/jeyan/foundpose/output_barrelddt1_raw_vitl_layer18\")\n",
    "dataset_path = Path(\"/scratch/jeyan/barreldata/divedata/dive8/barrelddt1/rgb\")\n",
    "mask_path = Path(\"/scratch/jeyan/barreldata/results/barrelddt1/masks\")\n",
    "cam_json_path = Path(\"/scratch/jeyan/barreldata/divedata/dive8/barrelddt1/camera.json\")\n",
    "crop_size = (420, 420)\n",
    "grid_cell_size = 14.0\n",
    "crop_rel_pad = 0.2\n",
    "\n",
    "imgpaths = sorted(list(dataset_path.glob(\"*.jpg\")) + list(dataset_path.glob(\"*.png\")))\n",
    "maskpaths = sorted(list(mask_path.glob(\"*.png\")))\n",
    "\n",
    "# Prepare feature extractor.\n",
    "extractor = feature_util.make_feature_extractor(extractor_name)\n",
    "# Prepare a device.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "extractor.to(device)\n",
    "\n",
    "# Create a pose evaluator.\n",
    "pose_evaluator = eval_util.EvaluatorPose([0])\n",
    "\n",
    "# Load the object representation.\n",
    "# logger.info(\n",
    "#     f\"Loading representation for object {0} from dataset {opts.object_dataset}...\"\n",
    "# )\n",
    "base_repre_dir = Path(output_path, \"object_repre\")\n",
    "repre_dir = base_repre_dir\n",
    "repre: repre_util.FeatureBasedObjectRepre = repre_util.load_object_repre(\n",
    "    repre_dir=repre_dir,\n",
    "    tensor_device=device,\n",
    ")\n",
    "\n",
    "repre_np = repre_util.convert_object_repre_to_numpy(repre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repre_np.feat_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feattempmask = repre_np.feat_to_template_ids == 0\n",
    "# all p_i values\n",
    "repre_np.feat_vectors[feattempmask].shape\n",
    "# all x_i values\n",
    "repre_np.vertices[repre_np.feat_to_vertex_ids[feattempmask]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repre_np.vertices[repre_np.feat_to_vertex_ids[feattempmask]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "imgpath = imgpaths[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Camera parameters.\n",
    "# transform is from GT, can we just leave as identity?\n",
    "with open(Path(cam_json_path), \"r\") as f:\n",
    "    camjson = json.load(f)\n",
    "orig_camera_c2w = PinholePlaneCameraModel(\n",
    "    camjson[\"width\"], camjson[\"height\"],\n",
    "    (camjson[\"fx\"], camjson[\"fy\"]), (camjson[\"cx\"], camjson[\"cy\"])\n",
    ")\n",
    "orig_image_size = (\n",
    "    orig_camera_c2w.width,\n",
    "    orig_camera_c2w.height,\n",
    ")\n",
    "\n",
    "# Generate grid points at which to sample the feature vectors.\n",
    "grid_size = crop_size\n",
    "grid_points = feature_util.generate_grid_points(\n",
    "    grid_size=grid_size,\n",
    "    cell_size=grid_cell_size,\n",
    ")\n",
    "grid_points = grid_points.to(device)\n",
    "\n",
    "\n",
    "# Estimate pose for each object instance.\n",
    "times = {}\n",
    "\n",
    "# Get the input image.\n",
    "orig_image_np_hwc = np.array(Image.open(imgpath)) / 255.0\n",
    "\n",
    "# Get the modal mask and amodal bounding box of the instance.\n",
    "# binary mask\n",
    "orig_mask_modal = np.array(Image.open(maskpaths[i]).convert(\"L\")) / 255.0\n",
    "sumvert = np.sum(orig_mask_modal, axis=0)\n",
    "left = np.where(sumvert > 0)[0][0]\n",
    "right = np.where(sumvert > 0)[0][-1]\n",
    "sumhor = np.sum(orig_mask_modal, axis=1)\n",
    "bottom = np.where(sumhor > 0)[0][0]\n",
    "top = np.where(sumhor > 0)[0][-1]\n",
    "# bounding box of mask\n",
    "orig_box_amodal = AlignedBox2f(\n",
    "    left=left,\n",
    "    top=top,\n",
    "    right=right,\n",
    "    bottom=bottom,\n",
    ")\n",
    "\n",
    "# Get box for cropping.\n",
    "crop_box = misc_util.calc_crop_box(\n",
    "    box=orig_box_amodal,\n",
    "    make_square=True,\n",
    ")\n",
    "\n",
    "# Construct a virtual camera focused on the crop.\n",
    "crop_camera_model_c2w = misc_util.construct_crop_camera(\n",
    "    box=crop_box,\n",
    "    camera_model_c2w=orig_camera_c2w,\n",
    "    viewport_size=crop_size,\n",
    "    viewport_rel_pad=crop_rel_pad,\n",
    ")\n",
    "\n",
    "# Map images to the virtual camera.\n",
    "interpolation = (\n",
    "    cv2.INTER_AREA\n",
    "    if crop_box.width >= crop_camera_model_c2w.width\n",
    "    else cv2.INTER_LINEAR\n",
    ")\n",
    "image_np_hwc = warp_image(\n",
    "    src_camera=orig_camera_c2w,\n",
    "    dst_camera=crop_camera_model_c2w,\n",
    "    src_image=orig_image_np_hwc,\n",
    "    interpolation=interpolation,\n",
    ")\n",
    "mask_modal = warp_image(\n",
    "    src_camera=orig_camera_c2w,\n",
    "    dst_camera=crop_camera_model_c2w,\n",
    "    src_image=orig_mask_modal,\n",
    "    interpolation=cv2.INTER_NEAREST,\n",
    ")\n",
    "\n",
    "# Recalculate the object bounding box (it changed if we constructed the virtual camera).\n",
    "ys, xs = mask_modal.nonzero()\n",
    "box = np.array(misc_util.calc_2d_box(xs, ys))\n",
    "box_amodal = AlignedBox2f(\n",
    "    left=box[0],\n",
    "    top=box[1],\n",
    "    right=box[2],\n",
    "    bottom=box[3],\n",
    ")\n",
    "\n",
    "# The virtual camera is becoming the main camera.\n",
    "camera_c2w = crop_camera_model_c2w\n",
    "\n",
    "# Extract feature map from the crop.\n",
    "image_tensor_chw = array_to_tensor(image_np_hwc).to(torch.float32).permute(2, 0, 1).to(device)\n",
    "image_tensor_bchw = image_tensor_chw.unsqueeze(0)\n",
    "# BxDxHxW\n",
    "extractor_output = extractor(image_tensor_bchw)\n",
    "feature_map_chw = extractor_output[\"feature_maps\"][0]\n",
    "\n",
    "# Keep only points inside the object mask.\n",
    "mask_modal_tensor = array_to_tensor(mask_modal).to(device)\n",
    "query_points = feature_util.filter_points_by_mask(\n",
    "    grid_points, mask_modal_tensor\n",
    ")\n",
    "\n",
    "# Extract features at the selected points, of shape (num_points, feat_dims).\n",
    "query_features = feature_util.sample_feature_map_at_points(\n",
    "    feature_map_chw=feature_map_chw,\n",
    "    points=query_points,\n",
    "    image_size=(image_np_hwc.shape[1], image_np_hwc.shape[0]),\n",
    ").contiguous()\n",
    "\n",
    "# Potentially project features to a PCA space.\n",
    "if (\n",
    "    query_features.shape[1] != repre.feat_vectors.shape[1]\n",
    "    and len(repre.feat_raw_projectors) != 0\n",
    "):\n",
    "    query_features_proj = projector_util.project_features(\n",
    "        feat_vectors=query_features,\n",
    "        projectors=repre.feat_raw_projectors,\n",
    "    ).contiguous()\n",
    "\n",
    "    _c, _h, _w = feature_map_chw.shape\n",
    "    feature_map_chw_proj = (\n",
    "        projector_util.project_features(\n",
    "            feat_vectors=feature_map_chw.permute(1, 2, 0).view(-1, _c),\n",
    "            projectors=repre.feat_raw_projectors,\n",
    "        )\n",
    "        .view(_h, _w, -1)\n",
    "        .permute(2, 0, 1)\n",
    "    )\n",
    "else:\n",
    "    query_features_proj = query_features\n",
    "    feature_map_chw_proj = feature_map_chw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_map_chw.shape, feature_map_chw_proj.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_features_proj.shape, feature_map_chw_proj.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = np.array([\n",
    "    [0.9824963517866108, -0.18626298081482784, -0.0026496611057800646],\n",
    "    [0.07184429208626533, 0.36576270685794776, 0.9279310534552504],\n",
    "    [-0.1718700567889119, -0.9118792377557032, 0.37274245710604814]\n",
    "])\n",
    "t = np.array([0.027086201383687152, 0.21964677626568319, 5.018856479067855])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_id = 245\n",
    "feattempmask = repre.feat_to_template_ids == template_id\n",
    "# all p_i values\n",
    "allpi = repre.feat_vectors[feattempmask]\n",
    "# all x_i values\n",
    "allxi = repre.vertices[feattempmask]\n",
    "Fq = feature_map_chw_proj\n",
    "templatecam = repre.template_cameras_cam_from_model[template_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bilinear_interpolate_torch(im, x, y):\n",
    "    \"\"\"ripped from https://gist.github.com/peteflorence/a1da2c759ca1ac2b74af9a83f69ce20e\"\"\"\n",
    "    x0 = torch.floor(x).long()\n",
    "    x1 = x0 + 1\n",
    "\n",
    "    y0 = torch.floor(y).long()\n",
    "    y1 = y0 + 1\n",
    "\n",
    "    # doesn't deal with edge and out of bounds, i don't care though\n",
    "    x0 = torch.clamp(x0, 0, im.shape[1] - 1)\n",
    "    x1 = torch.clamp(x1, 0, im.shape[1] - 1)\n",
    "    y0 = torch.clamp(y0, 0, im.shape[0] - 1)\n",
    "    y1 = torch.clamp(y1, 0, im.shape[0] - 1)\n",
    "\n",
    "    Ia = im[y0, x0]\n",
    "    Ib = im[y1, x0]\n",
    "    Ic = im[y0, x1]\n",
    "    Id = im[y1, x1]\n",
    "\n",
    "    wa = (x1.float() - x) * (y1.float() - y)\n",
    "    wb = (x1.float() - x) * (y - y0.float())\n",
    "    wc = (x - x0.float()) * (y1.float() - y)\n",
    "    wd = (x - x0.float()) * (y - y0.float())\n",
    "\n",
    "    return torch.t((torch.t(Ia) * wa)) + torch.t(torch.t(Ib) * wb) + torch.t(torch.t(Ic) * wc) + torch.t(torch.t(Id) * wd)\n",
    "\n",
    "def robustlosstorch(x: torch.Tensor, a=-5, c=0.5):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x (torch.Tensor): Nxd tensor\n",
    "    \n",
    "    Returns:\n",
    "        loss (torch.Tensor): N tensor\n",
    "    \"\"\"\n",
    "    return torch.sum((abs(a - 2) / a) * (((x / c) ** 2 / abs(a - 2) + 1) ** (a / 2) - 1), dim=-1)\n",
    "\n",
    "\n",
    "def descriptor_from_pose(\n",
    "    q: torch.Tensor, t: torch.Tensor, camera: CameraModel, patchvtxs: torch.Tensor,\n",
    "    featmap: torch.Tensor, patchsize: int, device=None\n",
    "):\n",
    "    \"\"\"\n",
    "    $$F_q(\\pi(Rx_i+t)/s)$$\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    T = roma.RigidUnitQuat(q, t)\n",
    "    # cropped camera has a unique transform, close to identity\n",
    "    world2eye = v3d.Transform.from_matrix(camera.T_world_from_eye).inv\n",
    "    qcam = roma.rotmat_to_unitquat(torch.tensor(world2eye.R)).float().to(device)\n",
    "    camT = roma.RigidUnitQuat(qcam, torch.tensor(world2eye.t).float().to(device))\n",
    "    # need to convert (fx,fy), (cx,cy) to tensors\n",
    "    camf = torch.tensor(camera.f).float().to(device)\n",
    "    camc = torch.tensor(camera.c).float().to(device)\n",
    "    # consider first transform to still be world space (identity camera pose)\n",
    "    # then transform to cropped camera space\n",
    "    camvtxs = camT[None].apply(T[None].apply(patchvtxs))\n",
    "    projected = camera.project(camvtxs) * camf + camc\n",
    "    patchproj = projected / patchsize\n",
    "    # torch grid_sample doesn't play nicely with jacrev for some reason\n",
    "    # projfeatures = feature_util.sample_feature_map_at_points(featmap, patchproj, featmap.shape[1:])\n",
    "    projfeatures = bilinear_interpolate_torch(featmap.permute(1, 2, 0), patchproj[:, 0], patchproj[:, 1])\n",
    "    return projfeatures\n",
    "\n",
    "\n",
    "def featuremetric_loss(\n",
    "    q: torch.Tensor, t: torch.Tensor, camera: CameraModel, patchdescs: torch.Tensor,\n",
    "    patchvtxs: torch.Tensor, featmap: torch.Tensor, patchsize: int, a: float=-5, c: float=0.5,\n",
    "    device=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        q: quaternion input\n",
    "        t: translation input\n",
    "        patchdescs (nxd)\n",
    "        patchvtxs (nx3)\n",
    "        featmap (dxaxa)\n",
    "\n",
    "    Returns:\n",
    "        loss (n tensor)\n",
    "    \"\"\"\n",
    "    # need: projected features from every patch in query rgb image\n",
    "    # template descriptor + 3d point from patches inside mask\n",
    "    # coarse R and t, need to parametrize R as quaternion or axangle\n",
    "    projfeatures = descriptor_from_pose(q, t, camera, patchvtxs, featmap, patchsize, device=device)\n",
    "    featurediff = patchdescs - projfeatures\n",
    "    return robustlosstorch(featurediff, a=a, c=c)\n",
    "\n",
    "\n",
    "def featuremetric_cost(\n",
    "    q: torch.Tensor, t: torch.Tensor, camera: CameraModel, patchdescs: torch.Tensor,\n",
    "    patchvtxs: torch.Tensor, featmap: torch.Tensor, patchsize: int, a: float=-5, c: float=0.5,\n",
    "    device=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        q: quaternion input\n",
    "        t: translation input\n",
    "    \"\"\"\n",
    "    losses = featuremetric_loss(q, t, camera, patchdescs, patchvtxs, featmap, patchsize, a=a, c=c, device=device)\n",
    "    return torch.sum(losses)\n",
    "\n",
    "class PoseDescriptorModel(nn.Module):\n",
    "    def __init__(self, R, t, camera: CameraModel, featmap: torch.Tensor, patchsize: int, device=None):\n",
    "        super().__init__()\n",
    "        if device is None:\n",
    "            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        else:\n",
    "            self.device = device\n",
    "        self.qtorch = nn.Parameter(roma.rotmat_to_unitquat(torch.tensor(R)).float().to(device), requires_grad=True)\n",
    "        self.ttorch = nn.Parameter(torch.tensor(t).float().to(device), requires_grad=True)\n",
    "        self.test = nn.Linear(256, 256)\n",
    "        self.camera = camera\n",
    "        self.featmap = featmap.to(device)\n",
    "        self.patchsize = patchsize\n",
    "\n",
    "    def forward(self, patchvtxs: torch.Tensor):\n",
    "        out = descriptor_from_pose(self.qtorch, self.ttorch, self.camera, patchvtxs, self.featmap, self.patchsize, device=self.device)\n",
    "        return self.test(out)\n",
    "        # return out\n",
    "\n",
    "class RobustLoss(tlm.loss.Loss):\n",
    "    def forward(self, y_true: torch.Tensor, y_pred: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.sum(torch.sqrt(robustlosstorch(y_true - y_pred)))\n",
    "\n",
    "    def residuals(self, y_true: torch.Tensor, y_pred: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.sqrt(robustlosstorch(y_true - y_pred))\n",
    "\n",
    "class PatchDataset(Dataset):\n",
    "    def __init__(self, patchvtxs: torch.Tensor, patchdescs: torch.Tensor, device=None):\n",
    "        if device is None:\n",
    "            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        else:\n",
    "            self.device = device\n",
    "        self.patchvtxs = patchvtxs.to(device)  # nx3\n",
    "        self.patchdescs = patchdescs.to(device)  # nxd\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.patchvtxs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.patchvtxs[idx], self.patchdescs[idx]\n",
    "\n",
    "def tree_to_device(tree: Any, device: torch.device | str) -> Any:\n",
    "    \"\"\"Recursively move all tensor leaves in a pytree to the specified device.\n",
    "\n",
    "    Args:\n",
    "        tree: The pytree containing tensors and non-tensor leaves.\n",
    "        device: The target device (e.g. CPU or GPU) to move the tensors to.\n",
    "\n",
    "    Returns:\n",
    "        A new pytree with every tensor moved to the specified device.\n",
    "    \"\"\"\n",
    "    return tree_map(lambda x: x.to(device) if isinstance(x, torch.Tensor) else x, tree)\n",
    "\n",
    "def fit(\n",
    "    training_module,\n",
    "    dataloader: DataLoader,\n",
    "    epochs: int,\n",
    "    metrics: dict[str, Metric] | None = None,\n",
    "    overwrite_progress_bar: bool = True,\n",
    "    update_every_n_steps: int = 1,\n",
    ") -> None:\n",
    "    \"\"\"Fit function with support for TrainingModule and torchmetrics.\n",
    "\n",
    "    Trains the model for a specified number of epochs. It supports logging metrics using\n",
    "    `torchmetrics` and provides detailed progress tracking using `tqdm`.\n",
    "\n",
    "    Args:\n",
    "        training_module: A `TrainingModule` encapsulating the training logic.\n",
    "        dataloader: A PyTorch DataLoader.\n",
    "        epochs: The number of epochs.\n",
    "        metrics: Optional dict of torchmetrics.Metric objects.\n",
    "        overwrite_progress_bar: If True, mimic a single-line progress bar similar\n",
    "            to PyTorch Lightning (old bars overwritten).\n",
    "        update_every_n_steps: Update the progress bar and displayed logs every n steps.\n",
    "    \"\"\"\n",
    "    assert update_every_n_steps > 0\n",
    "    device = training_module.device\n",
    "    steps = len(dataloader)\n",
    "    stop_training = False\n",
    "\n",
    "    if metrics:\n",
    "        metrics = {name: metric.to(device) for name, metric in metrics.items()}\n",
    "\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        if stop_training:\n",
    "            break\n",
    "\n",
    "        # Create a new progress bar for this epoch\n",
    "        progress_bar = tqdm(\n",
    "            total=steps,\n",
    "            desc=f'Epoch {epoch + 1}/{epochs}',\n",
    "            leave=not overwrite_progress_bar,  # Leave bar if overwrite is False\n",
    "            dynamic_ncols=True,\n",
    "        )\n",
    "        total_loss = 0.0\n",
    "        steps_since_update = 0\n",
    "\n",
    "        for step, (inputs, targets) in enumerate(dataloader):\n",
    "            # Ensure that inputs and targets are on the same device as the model\n",
    "            inputs = tree_to_device(inputs, device)\n",
    "            targets = tree_to_device(targets, device)\n",
    "\n",
    "            # Perform a training step\n",
    "            outputs, loss, stop_training, logs = training_module.training_step(\n",
    "                inputs, targets\n",
    "            )\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Update metrics if provided\n",
    "            if metrics:\n",
    "                for name, metric in metrics.items():\n",
    "                    metric(outputs, targets)\n",
    "\n",
    "            # Format logs\n",
    "            formatted_logs = {'loss': f'{loss:.4e}'}\n",
    "            if metrics:\n",
    "                for name, metric in metrics.items():\n",
    "                    formatted_logs[name] = metric.compute().item()\n",
    "            for key, value in logs.items():\n",
    "                if isinstance(value, torch.Tensor):\n",
    "                    value = value.item()\n",
    "                formatted_logs[key] = (\n",
    "                    f'{value:.4e}' if isinstance(value, float) else str(value)\n",
    "                )\n",
    "\n",
    "            steps_since_update += 1\n",
    "            if (\n",
    "                steps_since_update == update_every_n_steps\n",
    "                or step == steps - 1\n",
    "                or stop_training\n",
    "            ):\n",
    "                # Update the progress bar and logs\n",
    "                progress_bar.update(steps_since_update)\n",
    "                progress_bar.set_postfix(formatted_logs)\n",
    "                steps_since_update = 0\n",
    "\n",
    "            if stop_training:\n",
    "                # End early, ensure progress bar remains visible\n",
    "                progress_bar.leave = True\n",
    "                break\n",
    "\n",
    "        with torch.no_grad():\n",
    "            qraw = training_module.model.qtorch.data\n",
    "            training_module.model.qtorch.data = qraw / torch.norm(qraw)\n",
    "        losses.append(total_loss)\n",
    "        # Reset metrics at the end of the epoch\n",
    "        if metrics:\n",
    "            for metric in metrics.values():\n",
    "                metric.reset()\n",
    "\n",
    "        # Epoch summary\n",
    "        avg_loss = total_loss / steps\n",
    "        if overwrite_progress_bar:\n",
    "            progress_bar.set_postfix({'epoch_avg_loss': f'{avg_loss:.4e}'})\n",
    "        else:\n",
    "            progress_bar.write(\n",
    "                f'Epoch {epoch + 1} complete. Average loss: {avg_loss:.4e}'\n",
    "            )\n",
    "\n",
    "        # Ensure the final progress bar is left visible\n",
    "        if epoch == epochs - 1 or stop_training:\n",
    "            progress_bar.leave = True\n",
    "\n",
    "        progress_bar.close()\n",
    "\n",
    "    # Final training summary\n",
    "    if overwrite_progress_bar:\n",
    "        print(f'Training complete. Final epoch average loss: {avg_loss:.4e}')\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:1\"\n",
    "patchloader = DataLoader(PatchDataset(allxi, allpi, device=device), batch_size=9999, shuffle=False)\n",
    "posemodel = PoseDescriptorModel(R, t, camera_c2w, Fq, 14, device=device).to(device)\n",
    "fit(\n",
    "  tlm.training.LevenbergMarquardtModule(\n",
    "    model=posemodel,\n",
    "    loss_fn=RobustLoss(),\n",
    "    learning_rate=0.1,\n",
    "    attempts_per_step=10,\n",
    "    solve_method=\"qr\",\n",
    "    use_vmap=False,\n",
    "  ),\n",
    "  patchloader,\n",
    "  epochs=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roma.rotmat_to_unitquat(torch.tensor(R)), t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posemodel.qtorch.data, posemodel.ttorch.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.func import jacrev\n",
    "x = torch.randn(5)\n",
    "jacobian = jacrev(torch.sin)(x)\n",
    "expected = torch.diag(torch.cos(x))\n",
    "assert torch.allclose(jacobian, expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jacobian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(posemodel.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qtorch = roma.rotmat_to_unitquat(torch.tensor(R)).float().cuda().requires_grad_()\n",
    "ttorch = torch.tensor(t).float().cuda().requires_grad_()\n",
    "print(qtorch, ttorch)\n",
    "optimizer = torch.optim.Adam([qtorch, ttorch], lr=0.1)\n",
    "\n",
    "losses = []\n",
    "for _ in range(100):\n",
    "    loss = featuremetric_cost(qtorch, ttorch, camera_c2w, allpi, allxi, Fq, 14)\n",
    "    losses.append(loss.cpu().detach().numpy())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    with torch.no_grad():\n",
    "        qtorch = roma.quat_normalize(qtorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qtorch, ttorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repre.vertices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_camera_c2w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = roma.rotmat_to_unitquat(torch.tensor(R)).float().cuda()\n",
    "t = torch.tensor(t).float().cuda()\n",
    "T = roma.RigidUnitQuat(q, t)\n",
    "# cam = orig_camera_c2w\n",
    "cam = camera_c2w\n",
    "camera_c2w.T_world_from_eye\n",
    "camf = torch.tensor(cam.f).float().cuda()\n",
    "camc = torch.tensor(cam.c).float().cuda()\n",
    "projected = cam.project(T[None].apply(repre.vertices)) * camf + camc\n",
    "projected = projected.cpu().numpy()\n",
    "plt.plot(projected[:, 1], projected[:, 0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world2eye = v3d.Transform.from_matrix(camera_c2w.T_world_from_eye).inv\n",
    "qcam = roma.rotmat_to_unitquat(torch.tensor(world2eye.R)).float().cuda()\n",
    "qcam\n",
    "camT = roma.RigidUnitQuat(qcam, torch.tensor(world2eye.t).float().cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera_c2w.T_world_from_eye"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_map_chw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_np_hwc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "foundpose_gpu_311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
