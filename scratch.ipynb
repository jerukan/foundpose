{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import gc\n",
    "import time\n",
    "import struct\n",
    "\n",
    "from typing import List, NamedTuple, Optional, Tuple\n",
    "\n",
    "import cv2\n",
    "import imageio\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import roma\n",
    "# import torch_levenberg_marquardt as tlm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(os.path.join(\"external\", \"bop_toolkit\")))\n",
    "from bop_toolkit_lib import inout\n",
    "sys.path.append(os.path.abspath(os.path.join(\"external\", \"dinov2\")))\n",
    "\n",
    "from utils.misc import array_to_tensor, tensor_to_array, tensors_to_arrays\n",
    "\n",
    "\n",
    "from utils import (\n",
    "    corresp_util,\n",
    "    config_util,\n",
    "    eval_errors,\n",
    "    eval_util,\n",
    "    feature_util,\n",
    "    infer_pose_util,\n",
    "    knn_util,\n",
    "    misc as misc_util,\n",
    "    pnp_util,\n",
    "    projector_util,\n",
    "    repre_util,\n",
    "    vis_util,\n",
    "    data_util,\n",
    "    renderer_builder,\n",
    "    json_util, \n",
    "    logging,\n",
    "    misc,\n",
    "    structs,\n",
    "    geometry\n",
    ")\n",
    "\n",
    "from utils.structs import AlignedBox2f, PinholePlaneCameraModel, CameraModel\n",
    "from utils.misc import warp_depth_image, warp_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor_name = \"dinov2_version=vitl14-reg_stride=14_facet=token_layer=18_logbin=0_norm=1\"\n",
    "output_path = Path(\"/scratch/jeyan/foundpose/output_barrelddt1_raw_vitl_layer18\")\n",
    "dataset_path = Path(\"/scratch/jeyan/barreldata/divedata/dive8/barrelddt1/rgb\")\n",
    "mask_path = Path(\"/scratch/jeyan/barreldata/results/barrelddt1/masks\")\n",
    "cam_json_path = Path(\"/scratch/jeyan/barreldata/divedata/dive8/barrelddt1/camera.json\")\n",
    "crop_size = (420, 420)\n",
    "grid_cell_size = 14.0\n",
    "crop_rel_pad = 0.2\n",
    "\n",
    "imgpaths = sorted(list(dataset_path.glob(\"*.jpg\")) + list(dataset_path.glob(\"*.png\")))\n",
    "maskpaths = sorted(list(mask_path.glob(\"*.png\")))\n",
    "\n",
    "# Prepare feature extractor.\n",
    "extractor = feature_util.make_feature_extractor(extractor_name)\n",
    "# Prepare a device.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "extractor.to(device)\n",
    "\n",
    "# Create a pose evaluator.\n",
    "pose_evaluator = eval_util.EvaluatorPose([0])\n",
    "\n",
    "# Load the object representation.\n",
    "# logger.info(\n",
    "#     f\"Loading representation for object {0} from dataset {opts.object_dataset}...\"\n",
    "# )\n",
    "base_repre_dir = Path(output_path, \"object_repre\")\n",
    "repre_dir = base_repre_dir\n",
    "repre: repre_util.FeatureBasedObjectRepre = repre_util.load_object_repre(\n",
    "    repre_dir=repre_dir,\n",
    "    tensor_device=device,\n",
    ")\n",
    "\n",
    "repre_np = repre_util.convert_object_repre_to_numpy(repre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repre_np.feat_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feattempmask = repre_np.feat_to_template_ids == 0\n",
    "# all p_i values\n",
    "repre_np.feat_vectors[feattempmask].shape\n",
    "# all x_i values\n",
    "repre_np.vertices[repre_np.feat_to_vertex_ids[feattempmask]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repre_np.vertices[repre_np.feat_to_vertex_ids[feattempmask]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "imgpath = imgpaths[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Camera parameters.\n",
    "# transform is from GT, can we just leave as identity?\n",
    "with open(Path(cam_json_path), \"r\") as f:\n",
    "    camjson = json.load(f)\n",
    "orig_camera_c2w = PinholePlaneCameraModel(\n",
    "    camjson[\"width\"], camjson[\"height\"],\n",
    "    (camjson[\"fx\"], camjson[\"fy\"]), (camjson[\"cx\"], camjson[\"cy\"])\n",
    ")\n",
    "orig_image_size = (\n",
    "    orig_camera_c2w.width,\n",
    "    orig_camera_c2w.height,\n",
    ")\n",
    "\n",
    "# Generate grid points at which to sample the feature vectors.\n",
    "grid_size = crop_size\n",
    "grid_points = feature_util.generate_grid_points(\n",
    "    grid_size=grid_size,\n",
    "    cell_size=grid_cell_size,\n",
    ")\n",
    "grid_points = grid_points.to(device)\n",
    "\n",
    "\n",
    "# Estimate pose for each object instance.\n",
    "times = {}\n",
    "\n",
    "# Get the input image.\n",
    "orig_image_np_hwc = np.array(Image.open(imgpath)) / 255.0\n",
    "\n",
    "# Get the modal mask and amodal bounding box of the instance.\n",
    "# binary mask\n",
    "orig_mask_modal = np.array(Image.open(maskpaths[i]).convert(\"L\")) / 255.0\n",
    "sumvert = np.sum(orig_mask_modal, axis=0)\n",
    "left = np.where(sumvert > 0)[0][0]\n",
    "right = np.where(sumvert > 0)[0][-1]\n",
    "sumhor = np.sum(orig_mask_modal, axis=1)\n",
    "bottom = np.where(sumhor > 0)[0][0]\n",
    "top = np.where(sumhor > 0)[0][-1]\n",
    "# bounding box of mask\n",
    "orig_box_amodal = AlignedBox2f(\n",
    "    left=left,\n",
    "    top=top,\n",
    "    right=right,\n",
    "    bottom=bottom,\n",
    ")\n",
    "\n",
    "# Get box for cropping.\n",
    "crop_box = misc_util.calc_crop_box(\n",
    "    box=orig_box_amodal,\n",
    "    make_square=True,\n",
    ")\n",
    "\n",
    "# Construct a virtual camera focused on the crop.\n",
    "crop_camera_model_c2w = misc_util.construct_crop_camera(\n",
    "    box=crop_box,\n",
    "    camera_model_c2w=orig_camera_c2w,\n",
    "    viewport_size=crop_size,\n",
    "    viewport_rel_pad=crop_rel_pad,\n",
    ")\n",
    "\n",
    "# Map images to the virtual camera.\n",
    "interpolation = (\n",
    "    cv2.INTER_AREA\n",
    "    if crop_box.width >= crop_camera_model_c2w.width\n",
    "    else cv2.INTER_LINEAR\n",
    ")\n",
    "image_np_hwc = warp_image(\n",
    "    src_camera=orig_camera_c2w,\n",
    "    dst_camera=crop_camera_model_c2w,\n",
    "    src_image=orig_image_np_hwc,\n",
    "    interpolation=interpolation,\n",
    ")\n",
    "mask_modal = warp_image(\n",
    "    src_camera=orig_camera_c2w,\n",
    "    dst_camera=crop_camera_model_c2w,\n",
    "    src_image=orig_mask_modal,\n",
    "    interpolation=cv2.INTER_NEAREST,\n",
    ")\n",
    "\n",
    "# Recalculate the object bounding box (it changed if we constructed the virtual camera).\n",
    "ys, xs = mask_modal.nonzero()\n",
    "box = np.array(misc_util.calc_2d_box(xs, ys))\n",
    "box_amodal = AlignedBox2f(\n",
    "    left=box[0],\n",
    "    top=box[1],\n",
    "    right=box[2],\n",
    "    bottom=box[3],\n",
    ")\n",
    "\n",
    "# The virtual camera is becoming the main camera.\n",
    "camera_c2w = crop_camera_model_c2w\n",
    "\n",
    "# Extract feature map from the crop.\n",
    "image_tensor_chw = array_to_tensor(image_np_hwc).to(torch.float32).permute(2, 0, 1).to(device)\n",
    "image_tensor_bchw = image_tensor_chw.unsqueeze(0)\n",
    "# BxDxHxW\n",
    "extractor_output = extractor(image_tensor_bchw)\n",
    "feature_map_chw = extractor_output[\"feature_maps\"][0]\n",
    "\n",
    "# Keep only points inside the object mask.\n",
    "mask_modal_tensor = array_to_tensor(mask_modal).to(device)\n",
    "query_points = feature_util.filter_points_by_mask(\n",
    "    grid_points, mask_modal_tensor\n",
    ")\n",
    "\n",
    "# Extract features at the selected points, of shape (num_points, feat_dims).\n",
    "query_features = feature_util.sample_feature_map_at_points(\n",
    "    feature_map_chw=feature_map_chw,\n",
    "    points=query_points,\n",
    "    image_size=(image_np_hwc.shape[1], image_np_hwc.shape[0]),\n",
    ").contiguous()\n",
    "\n",
    "# Potentially project features to a PCA space.\n",
    "if (\n",
    "    query_features.shape[1] != repre.feat_vectors.shape[1]\n",
    "    and len(repre.feat_raw_projectors) != 0\n",
    "):\n",
    "    query_features_proj = projector_util.project_features(\n",
    "        feat_vectors=query_features,\n",
    "        projectors=repre.feat_raw_projectors,\n",
    "    ).contiguous()\n",
    "\n",
    "    _c, _h, _w = feature_map_chw.shape\n",
    "    feature_map_chw_proj = (\n",
    "        projector_util.project_features(\n",
    "            feat_vectors=feature_map_chw.permute(1, 2, 0).view(-1, _c),\n",
    "            projectors=repre.feat_raw_projectors,\n",
    "        )\n",
    "        .view(_h, _w, -1)\n",
    "        .permute(2, 0, 1)\n",
    "    )\n",
    "else:\n",
    "    query_features_proj = query_features\n",
    "    feature_map_chw_proj = feature_map_chw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_map_chw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_features_proj.shape, feature_map_chw_proj.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = np.array([\n",
    "    [0.9824963517866108, -0.18626298081482784, -0.0026496611057800646],\n",
    "    [0.07184429208626533, 0.36576270685794776, 0.9279310534552504],\n",
    "    [-0.1718700567889119, -0.9118792377557032, 0.37274245710604814]\n",
    "])\n",
    "t = np.array([0.027086201383687152, 0.21964677626568319, 5.018856479067855])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_id = 245\n",
    "feattempmask = repre.feat_to_template_ids == template_id\n",
    "# all p_i values\n",
    "allpi = repre.feat_vectors[feattempmask]\n",
    "# all x_i values\n",
    "allxi = repre.vertices[feattempmask]\n",
    "Fq = feature_map_chw_proj\n",
    "templatecam = repre.template_cameras_cam_from_model[template_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def robustlosstorch(x: torch.Tensor, a=-5, c=0.5):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x (torch.Tensor): Nxd tensor\n",
    "    \n",
    "    Returns:\n",
    "        loss (torch.Tensor): N tensor\n",
    "    \"\"\"\n",
    "    return torch.sum((abs(a - 2) / a) * (((x / c) ** 2 / abs(a - 2) + 1) ** (a / 2) - 1), dim=-1)\n",
    "\n",
    "def descriptor_from_pose(\n",
    "    q: torch.Tensor, t: torch.Tensor, camera: CameraModel, patchvtxs: torch.Tensor,\n",
    "    featmap: torch.Tensor, patchsize: int\n",
    "):\n",
    "    T = roma.RigidUnitQuat(q, t)\n",
    "    camf = torch.tensor(camera.f).cuda()\n",
    "    camc = torch.tensor(camera.c).cuda()\n",
    "    projected = camera.project(T[None].apply(patchvtxs)) * camf + camc\n",
    "    patchproj = projected / patchsize\n",
    "    projfeatures = feature_util.sample_feature_map_at_points(featmap, patchproj, featmap.shape[1:])\n",
    "    return projfeatures\n",
    "\n",
    "def featuremetric_loss(\n",
    "    q: torch.Tensor, t: torch.Tensor, camera: CameraModel, patchdescs: torch.Tensor,\n",
    "    patchvtxs: torch.Tensor, featmap: torch.Tensor, patchsize: int, a: float=-5, c: float=0.5\n",
    "):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        q: quaternion input\n",
    "        t: translation input\n",
    "    \"\"\"\n",
    "    # need: projected features from every patch in query rgb image\n",
    "    # template descriptor + 3d point from patches inside mask\n",
    "    # coarse R and t, need to parametrize R as quaternion or axangle\n",
    "    # tfm is 6d [*axangle, *t]\n",
    "    projfeatures = descriptor_from_pose(q, t, camera, patchvtxs, featmap, patchsize)\n",
    "    featurediff = patchdescs - projfeatures\n",
    "    return robustlosstorch(featurediff, a=a, c=c)\n",
    "\n",
    "def featuremetric_cost(\n",
    "    q: torch.Tensor, t: torch.Tensor, camera: CameraModel, patchdescs: torch.Tensor,\n",
    "    patchvtxs: torch.Tensor, featmap: torch.Tensor, patchsize: int, a: float=-5, c: float=0.5\n",
    "):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        q: quaternion input\n",
    "        t: translation input\n",
    "    \"\"\"\n",
    "    return torch.sum(featuremetric_loss(q, t, camera, patchdescs, patchvtxs, featmap, patchsize, a=a, c=c))\n",
    "\n",
    "class PoseDescriptorModel(nn.Module):\n",
    "    def __init__(self, q: torch.Tensor, t: torch.Tensor, camera: CameraModel, patchvtxs: torch.Tensor, featmap: torch.Tensor, patchsize: int):\n",
    "        super().__init__()\n",
    "        self.qtorch = roma.rotmat_to_unitquat(torch.tensor(R)).float().cuda().requires_grad_()\n",
    "        self.ttorch = torch.tensor(t).float().cuda().requires_grad_()\n",
    "        self.camera = camera\n",
    "        self.featmap = featmap\n",
    "        self.patchsize = patchsize\n",
    "\n",
    "    def forward(self, patchvtxs: torch.Tensor):\n",
    "        return descriptor_from_pose(self.qtorch, self.ttorch, self.camera, patchvtxs, self.featmap, self.patchsize)\n",
    "\n",
    "# class RobustLoss(tlm.loss.Loss):\n",
    "#     def forward(self, y_true: torch.Tensor, y_pred: torch.Tensor) -> torch.Tensor:\n",
    "#         return torch.sum(robustlosstorch(y_true - y_pred))\n",
    "\n",
    "#     def residuals(self, y_true: torch.Tensor, y_pred: torch.Tensor) -> torch.Tensor:\n",
    "#         return robustlosstorch(y_true - y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qtorch = roma.rotmat_to_unitquat(torch.tensor(R)).float().cuda().requires_grad_()\n",
    "ttorch = torch.tensor(t).float().cuda().requires_grad_()\n",
    "print(qtorch, ttorch)\n",
    "optimizer = torch.optim.Adam([qtorch, ttorch], lr=0.01)\n",
    "\n",
    "losses = []\n",
    "for _ in range(100):\n",
    "    loss = featuremetric_cost(qtorch, ttorch, camera_c2w, allpi, allxi, Fq, 14)\n",
    "    losses.append(loss.cpu().detach().numpy())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    with torch.no_grad():\n",
    "        qtorch = roma.quat_normalize(qtorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qtorch, ttorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_map_chw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_np_hwc.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "foundpose_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
